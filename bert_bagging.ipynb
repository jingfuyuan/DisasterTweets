{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPkZ2bou20vckaqkXZ/F/dr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9b155efa7a014c8bbed52a6b2895fcb8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b54ea3059c244637989e2296b98c4d65","IPY_MODEL_28daac7b973c43549f8dca81faa74f88","IPY_MODEL_0ceb44917f57430fa2f4e057da7280bc"],"layout":"IPY_MODEL_45c09ba433104fbcb5cef0265622a1f4"}},"b54ea3059c244637989e2296b98c4d65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1baaaa0a306347239704b7fda92f7a5c","placeholder":"​","style":"IPY_MODEL_70faea3a8c1a4712ae951a516741654e","value":"Downloading (…)okenizer_config.json: 100%"}},"28daac7b973c43549f8dca81faa74f88":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_40dc87932a3c4b7398b19919a8dad83d","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2466b30ecce4943b1c5bf4069ae0dca","value":28}},"0ceb44917f57430fa2f4e057da7280bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_840faf74315d456596d9f5e1f08e593a","placeholder":"​","style":"IPY_MODEL_679a7dc8f4dd4129bc2fa72ee002989c","value":" 28.0/28.0 [00:00&lt;00:00, 1.59kB/s]"}},"45c09ba433104fbcb5cef0265622a1f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1baaaa0a306347239704b7fda92f7a5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70faea3a8c1a4712ae951a516741654e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40dc87932a3c4b7398b19919a8dad83d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2466b30ecce4943b1c5bf4069ae0dca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"840faf74315d456596d9f5e1f08e593a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"679a7dc8f4dd4129bc2fa72ee002989c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a09e239ae104c2ab2a876ac05d6b590":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f9b0332fead479eb3db2354345b3562","IPY_MODEL_a5dd0749a0f64709b3b3778037202a2f","IPY_MODEL_fb806759537d472d8ad9985895fda76f"],"layout":"IPY_MODEL_195864f4bef6440fbf98854d768ade6c"}},"1f9b0332fead479eb3db2354345b3562":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_051887faff6d40bba83b0327c9e792f4","placeholder":"​","style":"IPY_MODEL_2c966b0e4e17451ab08b22ef17ef6744","value":"Downloading (…)lve/main/config.json: 100%"}},"a5dd0749a0f64709b3b3778037202a2f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_027aa1c462954fe7a7b7c7865e044ee2","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f3293bd98b894e218331383fe21834a0","value":570}},"fb806759537d472d8ad9985895fda76f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07d09a371b7f408498b4b0679989d1be","placeholder":"​","style":"IPY_MODEL_d1b3406fc49a4b7b8c1cab651ff2d5b5","value":" 570/570 [00:00&lt;00:00, 41.7kB/s]"}},"195864f4bef6440fbf98854d768ade6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"051887faff6d40bba83b0327c9e792f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c966b0e4e17451ab08b22ef17ef6744":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"027aa1c462954fe7a7b7c7865e044ee2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3293bd98b894e218331383fe21834a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"07d09a371b7f408498b4b0679989d1be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1b3406fc49a4b7b8c1cab651ff2d5b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b9186471cf5475d97e63ac8edbfbe9c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd4bc014be5347129e6a249fa2d9747c","IPY_MODEL_5c76199504924b16b540293563765dbb","IPY_MODEL_5e53e6b88beb411c9be17358586323a4"],"layout":"IPY_MODEL_f7905279cdf548dfb960ca07a2b663b5"}},"fd4bc014be5347129e6a249fa2d9747c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af4c27f0ebe34520948429650e913346","placeholder":"​","style":"IPY_MODEL_e1d4a259638f454787306af3036377b9","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"5c76199504924b16b540293563765dbb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_145760c3916e460099c8b52c0d02d009","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0aca5aa92494285b612f2c797672191","value":231508}},"5e53e6b88beb411c9be17358586323a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e706d72001464e5b8f4ee9220c4df4cc","placeholder":"​","style":"IPY_MODEL_637b4ec7d0974cbab595809e9c36915e","value":" 232k/232k [00:00&lt;00:00, 2.98MB/s]"}},"f7905279cdf548dfb960ca07a2b663b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af4c27f0ebe34520948429650e913346":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1d4a259638f454787306af3036377b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"145760c3916e460099c8b52c0d02d009":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0aca5aa92494285b612f2c797672191":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e706d72001464e5b8f4ee9220c4df4cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"637b4ec7d0974cbab595809e9c36915e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a4cb27baaa5e4f9195fdc957001f63cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fbd6f5914305473081c3e36bc18f7278","IPY_MODEL_6da5ed50b8de413c9db2cf0bd654b6a1","IPY_MODEL_82e75b8996e04e6e9239d810c1b777be"],"layout":"IPY_MODEL_57659aac30394c18852798a91ced6175"}},"fbd6f5914305473081c3e36bc18f7278":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b940593baa644a68c853de64504d445","placeholder":"​","style":"IPY_MODEL_5d8393851ea34d139c49c47c200a1318","value":"Downloading (…)/main/tokenizer.json: 100%"}},"6da5ed50b8de413c9db2cf0bd654b6a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a9dbcd70b2d4794867fb8b8a00f67d8","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7793f997a2494f4696342c7bf7592e8b","value":466062}},"82e75b8996e04e6e9239d810c1b777be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58cfe9917f1e4504a49ea4526d98240f","placeholder":"​","style":"IPY_MODEL_f02015597c3b4f71a54b978965ead472","value":" 466k/466k [00:00&lt;00:00, 5.01MB/s]"}},"57659aac30394c18852798a91ced6175":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b940593baa644a68c853de64504d445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d8393851ea34d139c49c47c200a1318":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a9dbcd70b2d4794867fb8b8a00f67d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7793f997a2494f4696342c7bf7592e8b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58cfe9917f1e4504a49ea4526d98240f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f02015597c3b4f71a54b978965ead472":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"414ab8f49a944a0b9f568232e3a551db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66988654b09d4dc2b169e3484702878f","IPY_MODEL_dcb0f0aa681a43c9bafdf311eca0d358","IPY_MODEL_597dfcdd5dd644caac986b4a1a47009d"],"layout":"IPY_MODEL_798b7b19e17e4b3d809eb4b5e27083bd"}},"66988654b09d4dc2b169e3484702878f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2161b05ef3234a70845428886fdb98d0","placeholder":"​","style":"IPY_MODEL_2038ebe87fc74c2f9d9106c7816249fe","value":"Downloading (…)&quot;tf_model.h5&quot;;: 100%"}},"dcb0f0aa681a43c9bafdf311eca0d358":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_711ac6b50fe94787af5aaa903455947a","max":536063208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_21686e40fa5b472e89996a9b9a8068bc","value":536063208}},"597dfcdd5dd644caac986b4a1a47009d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e8b3005a9ac4d45922e0e007a0b9cb5","placeholder":"​","style":"IPY_MODEL_4fc104b2df464bd4b0f7a72803caa88a","value":" 536M/536M [00:03&lt;00:00, 141MB/s]"}},"798b7b19e17e4b3d809eb4b5e27083bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2161b05ef3234a70845428886fdb98d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2038ebe87fc74c2f9d9106c7816249fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"711ac6b50fe94787af5aaa903455947a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21686e40fa5b472e89996a9b9a8068bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e8b3005a9ac4d45922e0e007a0b9cb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fc104b2df464bd4b0f7a72803caa88a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# NLP with Disaster Tweets (on Kaggle)\n","\n","In another notebook, I have compared different models in predicting disaster tweets. I found transformers BERT model give me the best accuracy. Here, I'm trying to use bagging to further improve the performance."],"metadata":{"id":"rY4h0d048I2G"}},{"cell_type":"markdown","source":["### Import libraries and import data"],"metadata":{"id":"M41-Ohnt-aIj"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v4CicMblbcM1","executionInfo":{"status":"ok","timestamp":1674928112214,"user_tz":300,"elapsed":16832,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}},"outputId":"2763911e-5268-45a2-f046-cf17694dcaa1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wq9xN3mJl_F-","executionInfo":{"status":"ok","timestamp":1674928121816,"user_tz":300,"elapsed":9605,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}},"outputId":"0fee40f6-2d5f-43f5-9c0f-eeeef8d9d225"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","import tensorflow.keras.layers as layers\n","from tensorflow.keras.optimizers import Adam, SGD\n","from tensorflow.keras.models import Model\n","from transformers import AutoTokenizer, TFAutoModel\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"Mg3oEIzYcJue","executionInfo":{"status":"ok","timestamp":1674928124682,"user_tz":300,"elapsed":2750,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["path = \"/content/drive/MyDrive/Colab Notebooks/DisasterTweets/data/\"\n","train_df = pd.read_csv(path + \"train.csv\")\n","test_df = pd.read_csv(path + \"test.csv\")"],"metadata":{"id":"D7_Ts-jYdPUh","executionInfo":{"status":"ok","timestamp":1674928125164,"user_tz":300,"elapsed":484,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import re"],"metadata":{"id":"JGkYYb2Ed-j-","executionInfo":{"status":"ok","timestamp":1674928125164,"user_tz":300,"elapsed":7,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["remove_url = lambda x:re.sub(r'https?://\\S+|www\\.\\S+', '', x)\n","train_df['text'] = train_df['text'].map(remove_url)\n","test_df['text'] = test_df['text'].map(remove_url)"],"metadata":{"id":"vTX-yQ3VdlXY","executionInfo":{"status":"ok","timestamp":1674928125165,"user_tz":300,"elapsed":8,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["train_sentences = train_df['text'].to_numpy()\n","test_sentences = test_df['text'].to_numpy()\n","y = train_df['target'].to_numpy()"],"metadata":{"id":"gLwtK3ojgawx","executionInfo":{"status":"ok","timestamp":1674928125165,"user_tz":300,"elapsed":7,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# number of training samples\n","n = len(train_sentences)\n","print(\"number of trainning samples: {}\".format(n))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PuJOzvSIgyu8","executionInfo":{"status":"ok","timestamp":1674928125165,"user_tz":300,"elapsed":7,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}},"outputId":"9cd21c44-9472-4348-b898-0a193594e6cb"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["number of trainning samples: 7613\n"]}]},{"cell_type":"markdown","source":["### Functions for sampling, tokenizer, and building models"],"metadata":{"id":"Vw7dIWCL-mtf"}},{"cell_type":"code","source":["# create a function to randomly choose samples\n","def resample(sample, target, sample_size=0.7):\n","  if not isinstance(sample, np.ndarray):\n","    sample = np.array(sample)\n","  if not isinstance(target, np.ndarray):\n","    target = np.array(target)\n","  n = len(sample)\n","  idx_selector = np.random.choice(n,int(sample_size*n), replace=False)\n","  not_selected = list(set(range(n)) - set(idx_selector))\n","  return sample[idx_selector], target[idx_selector], sample[not_selected], target[not_selected]"],"metadata":{"id":"TQqYnMzlhIm1","executionInfo":{"status":"ok","timestamp":1674928125166,"user_tz":300,"elapsed":6,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["checkpoint = 'bert-base-uncased'\n","max_length=33\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","\n","def tokenize_samples(samples):\n","  if isinstance(samples, np.ndarray):\n","    samples = samples.tolist()\n","  \n","  return tokenizer(\n","      text=samples,\n","      add_special_tokens=True,\n","      max_length=max_length,\n","      truncation=True,\n","      padding=True,\n","      return_tensors='tf',\n","      return_token_type_ids=False,\n","      return_attention_mask=True,\n","      verbose=True\n","    )\n","\n","X_test = tokenize_samples(test_sentences)"],"metadata":{"id":"HgEhUwf_cBw0","executionInfo":{"status":"ok","timestamp":1674928129709,"user_tz":300,"elapsed":4549,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}},"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["9b155efa7a014c8bbed52a6b2895fcb8","b54ea3059c244637989e2296b98c4d65","28daac7b973c43549f8dca81faa74f88","0ceb44917f57430fa2f4e057da7280bc","45c09ba433104fbcb5cef0265622a1f4","1baaaa0a306347239704b7fda92f7a5c","70faea3a8c1a4712ae951a516741654e","40dc87932a3c4b7398b19919a8dad83d","c2466b30ecce4943b1c5bf4069ae0dca","840faf74315d456596d9f5e1f08e593a","679a7dc8f4dd4129bc2fa72ee002989c","3a09e239ae104c2ab2a876ac05d6b590","1f9b0332fead479eb3db2354345b3562","a5dd0749a0f64709b3b3778037202a2f","fb806759537d472d8ad9985895fda76f","195864f4bef6440fbf98854d768ade6c","051887faff6d40bba83b0327c9e792f4","2c966b0e4e17451ab08b22ef17ef6744","027aa1c462954fe7a7b7c7865e044ee2","f3293bd98b894e218331383fe21834a0","07d09a371b7f408498b4b0679989d1be","d1b3406fc49a4b7b8c1cab651ff2d5b5","7b9186471cf5475d97e63ac8edbfbe9c","fd4bc014be5347129e6a249fa2d9747c","5c76199504924b16b540293563765dbb","5e53e6b88beb411c9be17358586323a4","f7905279cdf548dfb960ca07a2b663b5","af4c27f0ebe34520948429650e913346","e1d4a259638f454787306af3036377b9","145760c3916e460099c8b52c0d02d009","d0aca5aa92494285b612f2c797672191","e706d72001464e5b8f4ee9220c4df4cc","637b4ec7d0974cbab595809e9c36915e","a4cb27baaa5e4f9195fdc957001f63cd","fbd6f5914305473081c3e36bc18f7278","6da5ed50b8de413c9db2cf0bd654b6a1","82e75b8996e04e6e9239d810c1b777be","57659aac30394c18852798a91ced6175","6b940593baa644a68c853de64504d445","5d8393851ea34d139c49c47c200a1318","6a9dbcd70b2d4794867fb8b8a00f67d8","7793f997a2494f4696342c7bf7592e8b","58cfe9917f1e4504a49ea4526d98240f","f02015597c3b4f71a54b978965ead472"]},"outputId":"58e0508e-cf1a-4a2d-9be0-2e8c39b5a0a6"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b155efa7a014c8bbed52a6b2895fcb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a09e239ae104c2ab2a876ac05d6b590"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b9186471cf5475d97e63ac8edbfbe9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4cb27baaa5e4f9195fdc957001f63cd"}},"metadata":{}}]},{"cell_type":"code","source":["# write a function to create and compile model\n","def create_compile_model(checkpoint=checkpoint):\n","  pretrained_model = TFAutoModel.from_pretrained(checkpoint)\n","  input_ids = layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n","  input_mask = layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n","  embeddings = pretrained_model(input_ids, attention_mask = input_mask)[1]\n","  x = layers.Dropout(0.3)(embeddings)\n","  x = layers.Dense(64, activation='relu')(x)\n","  x = layers.Dropout(0.3)(x)\n","  x = layers.Dense(32, activation='relu')(x)\n","  output = layers.Dense(1, activation='sigmoid')(x)\n","  model = Model(inputs=[input_ids, input_mask], outputs=output)\n","  model.compile(\n","      loss='binary_crossentropy',\n","      optimizer = Adam(learning_rate=5e-5),\n","      metrics=['accuracy']\n","  )\n","  return model"],"metadata":{"id":"zXKRHtS6nbWU","executionInfo":{"status":"ok","timestamp":1674928129710,"user_tz":300,"elapsed":6,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Combine models to make prediction"],"metadata":{"id":"LK89RoCI_-N7"}},{"cell_type":"code","source":["models = []\n","y_preds = []\n","n_models = 15\n","for i in range(n_models):\n","  train_samples, y_train, val_samples, y_val = resample(train_sentences, y)\n","  X_train = tokenize_samples(train_samples)\n","  X_val = tokenize_samples(val_samples)\n","  model = create_compile_model()\n","  model.fit(\n","      [X_train['input_ids'], X_train['attention_mask']],\n","      y_train,\n","      epochs=10,\n","      validation_data = ([X_val['input_ids'], X_val['attention_mask']], y_val)\n","  )\n","  models.append(model)\n","  probs = model.predict([X_test['input_ids'], X_test['attention_mask']]).flatten()\n","  pred = (probs > 0.5).astype('int')\n","  y_preds.append(pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["414ab8f49a944a0b9f568232e3a551db","66988654b09d4dc2b169e3484702878f","dcb0f0aa681a43c9bafdf311eca0d358","597dfcdd5dd644caac986b4a1a47009d","798b7b19e17e4b3d809eb4b5e27083bd","2161b05ef3234a70845428886fdb98d0","2038ebe87fc74c2f9d9106c7816249fe","711ac6b50fe94787af5aaa903455947a","21686e40fa5b472e89996a9b9a8068bc","9e8b3005a9ac4d45922e0e007a0b9cb5","4fc104b2df464bd4b0f7a72803caa88a"]},"id":"KCCBS-K9ov9B","executionInfo":{"status":"ok","timestamp":1674930166739,"user_tz":300,"elapsed":2037034,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}},"outputId":"3a8b833b-b9be-409f-89c6-1f1cdd82103c"},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)\"tf_model.h5\";:   0%|          | 0.00/536M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"414ab8f49a944a0b9f568232e3a551db"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 30s 81ms/step - loss: 0.5059 - accuracy: 0.7686 - val_loss: 0.4554 - val_accuracy: 0.8170\n","Epoch 2/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.3517 - accuracy: 0.8636 - val_loss: 0.4443 - val_accuracy: 0.8301\n","Epoch 3/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.2562 - accuracy: 0.9045 - val_loss: 0.4419 - val_accuracy: 0.8301\n","Epoch 4/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.1705 - accuracy: 0.9422 - val_loss: 0.6286 - val_accuracy: 0.8091\n","Epoch 5/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.1093 - accuracy: 0.9602 - val_loss: 0.7540 - val_accuracy: 0.8017\n","Epoch 6/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.0716 - accuracy: 0.9726 - val_loss: 0.8113 - val_accuracy: 0.8095\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0742 - accuracy: 0.9698 - val_loss: 0.8537 - val_accuracy: 0.8012\n","Epoch 8/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.0760 - accuracy: 0.9713 - val_loss: 0.9075 - val_accuracy: 0.7745\n","Epoch 9/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.0634 - accuracy: 0.9749 - val_loss: 0.8367 - val_accuracy: 0.8257\n","Epoch 10/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.0442 - accuracy: 0.9803 - val_loss: 0.9993 - val_accuracy: 0.8174\n","102/102 [==============================] - 5s 27ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 28s 82ms/step - loss: 0.5095 - accuracy: 0.7666 - val_loss: 0.4223 - val_accuracy: 0.8218\n","Epoch 2/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.3577 - accuracy: 0.8585 - val_loss: 0.4513 - val_accuracy: 0.8262\n","Epoch 3/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.2515 - accuracy: 0.9077 - val_loss: 0.4897 - val_accuracy: 0.8205\n","Epoch 4/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1576 - accuracy: 0.9461 - val_loss: 0.6771 - val_accuracy: 0.7990\n","Epoch 5/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1191 - accuracy: 0.9570 - val_loss: 0.9036 - val_accuracy: 0.7837\n","Epoch 6/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0808 - accuracy: 0.9717 - val_loss: 0.7793 - val_accuracy: 0.7767\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0683 - accuracy: 0.9747 - val_loss: 0.7910 - val_accuracy: 0.8039\n","Epoch 8/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0613 - accuracy: 0.9764 - val_loss: 0.9839 - val_accuracy: 0.7990\n","Epoch 9/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0506 - accuracy: 0.9780 - val_loss: 1.1523 - val_accuracy: 0.7877\n","Epoch 10/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0568 - accuracy: 0.9786 - val_loss: 1.0363 - val_accuracy: 0.7890\n","102/102 [==============================] - 5s 27ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 28s 81ms/step - loss: 0.4892 - accuracy: 0.7716 - val_loss: 0.3755 - val_accuracy: 0.8472\n","Epoch 2/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.3505 - accuracy: 0.8692 - val_loss: 0.3885 - val_accuracy: 0.8485\n","Epoch 3/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.2496 - accuracy: 0.9077 - val_loss: 0.4155 - val_accuracy: 0.8236\n","Epoch 4/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1714 - accuracy: 0.9354 - val_loss: 0.4990 - val_accuracy: 0.8292\n","Epoch 5/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1044 - accuracy: 0.9643 - val_loss: 0.6250 - val_accuracy: 0.8279\n","Epoch 6/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0772 - accuracy: 0.9728 - val_loss: 0.7257 - val_accuracy: 0.8279\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0810 - accuracy: 0.9717 - val_loss: 0.8029 - val_accuracy: 0.8314\n","Epoch 8/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0807 - accuracy: 0.9672 - val_loss: 0.7335 - val_accuracy: 0.7859\n","Epoch 9/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0677 - accuracy: 0.9735 - val_loss: 0.6980 - val_accuracy: 0.8122\n","Epoch 10/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0565 - accuracy: 0.9792 - val_loss: 0.7525 - val_accuracy: 0.8266\n","102/102 [==============================] - 5s 26ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 28s 81ms/step - loss: 0.4938 - accuracy: 0.7833 - val_loss: 0.4191 - val_accuracy: 0.8209\n","Epoch 2/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.3566 - accuracy: 0.8640 - val_loss: 0.4047 - val_accuracy: 0.8306\n","Epoch 3/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.2644 - accuracy: 0.9073 - val_loss: 0.5288 - val_accuracy: 0.7798\n","Epoch 4/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1740 - accuracy: 0.9424 - val_loss: 0.5573 - val_accuracy: 0.8095\n","Epoch 5/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.1270 - accuracy: 0.9574 - val_loss: 0.6329 - val_accuracy: 0.8192\n","Epoch 6/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0965 - accuracy: 0.9642 - val_loss: 0.7328 - val_accuracy: 0.7986\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0649 - accuracy: 0.9741 - val_loss: 0.7450 - val_accuracy: 0.8056\n","Epoch 8/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0606 - accuracy: 0.9794 - val_loss: 0.9739 - val_accuracy: 0.7837\n","Epoch 9/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0597 - accuracy: 0.9782 - val_loss: 0.8322 - val_accuracy: 0.8165\n","Epoch 10/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0445 - accuracy: 0.9822 - val_loss: 1.0436 - val_accuracy: 0.8135\n","102/102 [==============================] - 5s 27ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 28s 81ms/step - loss: 0.5297 - accuracy: 0.7472 - val_loss: 0.4232 - val_accuracy: 0.8170\n","Epoch 2/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.3634 - accuracy: 0.8555 - val_loss: 0.5057 - val_accuracy: 0.8074\n","Epoch 3/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.2516 - accuracy: 0.9047 - val_loss: 0.5324 - val_accuracy: 0.8069\n","Epoch 4/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1567 - accuracy: 0.9394 - val_loss: 0.5556 - val_accuracy: 0.8209\n","Epoch 5/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.1076 - accuracy: 0.9621 - val_loss: 0.6544 - val_accuracy: 0.8161\n","Epoch 6/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0741 - accuracy: 0.9737 - val_loss: 0.8087 - val_accuracy: 0.8126\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0686 - accuracy: 0.9728 - val_loss: 1.0008 - val_accuracy: 0.7947\n","Epoch 8/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0642 - accuracy: 0.9765 - val_loss: 0.7431 - val_accuracy: 0.8192\n","Epoch 9/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0494 - accuracy: 0.9790 - val_loss: 1.0327 - val_accuracy: 0.7955\n","Epoch 10/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0435 - accuracy: 0.9786 - val_loss: 1.0646 - val_accuracy: 0.8209\n","102/102 [==============================] - 5s 26ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 28s 82ms/step - loss: 0.4869 - accuracy: 0.7778 - val_loss: 0.4219 - val_accuracy: 0.8122\n","Epoch 2/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.3559 - accuracy: 0.8617 - val_loss: 0.4219 - val_accuracy: 0.8205\n","Epoch 3/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.2493 - accuracy: 0.9105 - val_loss: 0.5131 - val_accuracy: 0.8201\n","Epoch 4/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1696 - accuracy: 0.9392 - val_loss: 0.4819 - val_accuracy: 0.8100\n","Epoch 5/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1275 - accuracy: 0.9540 - val_loss: 0.6373 - val_accuracy: 0.8179\n","Epoch 6/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0905 - accuracy: 0.9668 - val_loss: 0.7099 - val_accuracy: 0.7811\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0586 - accuracy: 0.9756 - val_loss: 0.9541 - val_accuracy: 0.8170\n","Epoch 8/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0685 - accuracy: 0.9745 - val_loss: 0.7829 - val_accuracy: 0.8109\n","Epoch 9/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0490 - accuracy: 0.9816 - val_loss: 0.9145 - val_accuracy: 0.8065\n","Epoch 10/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0491 - accuracy: 0.9794 - val_loss: 1.0344 - val_accuracy: 0.8052\n","102/102 [==============================] - 5s 27ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 28s 81ms/step - loss: 0.5130 - accuracy: 0.7624 - val_loss: 0.4164 - val_accuracy: 0.8227\n","Epoch 2/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.3619 - accuracy: 0.8581 - val_loss: 0.3860 - val_accuracy: 0.8419\n","Epoch 3/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.2659 - accuracy: 0.9043 - val_loss: 0.5024 - val_accuracy: 0.8078\n","Epoch 4/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1835 - accuracy: 0.9368 - val_loss: 0.6729 - val_accuracy: 0.7885\n","Epoch 5/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1294 - accuracy: 0.9557 - val_loss: 0.7397 - val_accuracy: 0.8039\n","Epoch 6/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1000 - accuracy: 0.9651 - val_loss: 0.6469 - val_accuracy: 0.7947\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0853 - accuracy: 0.9698 - val_loss: 0.6526 - val_accuracy: 0.8205\n","Epoch 8/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0650 - accuracy: 0.9732 - val_loss: 0.8920 - val_accuracy: 0.7872\n","Epoch 9/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0644 - accuracy: 0.9750 - val_loss: 0.8272 - val_accuracy: 0.8209\n","Epoch 10/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0462 - accuracy: 0.9803 - val_loss: 0.9435 - val_accuracy: 0.8113\n","102/102 [==============================] - 5s 27ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 28s 82ms/step - loss: 0.4890 - accuracy: 0.7750 - val_loss: 0.4123 - val_accuracy: 0.8113\n","Epoch 2/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.3423 - accuracy: 0.8694 - val_loss: 0.4179 - val_accuracy: 0.8222\n","Epoch 3/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.2342 - accuracy: 0.9167 - val_loss: 0.4908 - val_accuracy: 0.8222\n","Epoch 4/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1597 - accuracy: 0.9441 - val_loss: 0.6162 - val_accuracy: 0.8069\n","Epoch 5/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1023 - accuracy: 0.9653 - val_loss: 0.7995 - val_accuracy: 0.8144\n","Epoch 6/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0707 - accuracy: 0.9735 - val_loss: 0.8928 - val_accuracy: 0.8078\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0708 - accuracy: 0.9726 - val_loss: 0.7306 - val_accuracy: 0.8117\n","Epoch 8/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0647 - accuracy: 0.9741 - val_loss: 0.9347 - val_accuracy: 0.8078\n","Epoch 9/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0483 - accuracy: 0.9784 - val_loss: 1.0650 - val_accuracy: 0.8183\n","Epoch 10/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0463 - accuracy: 0.9812 - val_loss: 0.9593 - val_accuracy: 0.7982\n","102/102 [==============================] - 6s 26ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 28s 82ms/step - loss: 0.5231 - accuracy: 0.7546 - val_loss: 0.3950 - val_accuracy: 0.8380\n","Epoch 2/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.3699 - accuracy: 0.8609 - val_loss: 0.4056 - val_accuracy: 0.8433\n","Epoch 3/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.2609 - accuracy: 0.9073 - val_loss: 0.4602 - val_accuracy: 0.8165\n","Epoch 4/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1750 - accuracy: 0.9396 - val_loss: 0.5882 - val_accuracy: 0.8135\n","Epoch 5/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.1220 - accuracy: 0.9600 - val_loss: 0.7917 - val_accuracy: 0.8196\n","Epoch 6/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1303 - accuracy: 0.9561 - val_loss: 0.6076 - val_accuracy: 0.8165\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0870 - accuracy: 0.9681 - val_loss: 0.7024 - val_accuracy: 0.8060\n","Epoch 8/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0677 - accuracy: 0.9739 - val_loss: 0.8712 - val_accuracy: 0.8192\n","Epoch 9/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0535 - accuracy: 0.9788 - val_loss: 0.8895 - val_accuracy: 0.8196\n","Epoch 10/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0411 - accuracy: 0.9837 - val_loss: 1.0096 - val_accuracy: 0.8117\n","102/102 [==============================] - 5s 26ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 29s 81ms/step - loss: 0.5240 - accuracy: 0.7469 - val_loss: 0.4250 - val_accuracy: 0.8148\n","Epoch 2/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.3744 - accuracy: 0.8548 - val_loss: 0.3876 - val_accuracy: 0.8428\n","Epoch 3/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.2584 - accuracy: 0.9058 - val_loss: 0.5602 - val_accuracy: 0.8214\n","Epoch 4/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1815 - accuracy: 0.9415 - val_loss: 0.4884 - val_accuracy: 0.8275\n","Epoch 5/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1330 - accuracy: 0.9568 - val_loss: 0.6776 - val_accuracy: 0.8078\n","Epoch 6/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0949 - accuracy: 0.9645 - val_loss: 0.7001 - val_accuracy: 0.8205\n","Epoch 7/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.0536 - accuracy: 0.9779 - val_loss: 0.9873 - val_accuracy: 0.8253\n","Epoch 8/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0620 - accuracy: 0.9780 - val_loss: 0.7820 - val_accuracy: 0.8196\n","Epoch 9/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0506 - accuracy: 0.9809 - val_loss: 0.8767 - val_accuracy: 0.8130\n","Epoch 10/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0617 - accuracy: 0.9754 - val_loss: 0.7185 - val_accuracy: 0.8231\n","102/102 [==============================] - 5s 26ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 28s 81ms/step - loss: 0.5066 - accuracy: 0.7585 - val_loss: 0.4213 - val_accuracy: 0.8135\n","Epoch 2/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.3432 - accuracy: 0.8700 - val_loss: 0.4151 - val_accuracy: 0.8201\n","Epoch 3/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.2299 - accuracy: 0.9167 - val_loss: 0.4966 - val_accuracy: 0.8047\n","Epoch 4/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.1581 - accuracy: 0.9439 - val_loss: 0.5943 - val_accuracy: 0.8109\n","Epoch 5/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1049 - accuracy: 0.9617 - val_loss: 0.7711 - val_accuracy: 0.7929\n","Epoch 6/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0901 - accuracy: 0.9657 - val_loss: 0.6816 - val_accuracy: 0.7982\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0694 - accuracy: 0.9747 - val_loss: 0.6632 - val_accuracy: 0.8060\n","Epoch 8/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0635 - accuracy: 0.9784 - val_loss: 0.8480 - val_accuracy: 0.8065\n","Epoch 9/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0546 - accuracy: 0.9786 - val_loss: 0.8364 - val_accuracy: 0.7741\n","Epoch 10/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0574 - accuracy: 0.9780 - val_loss: 1.0174 - val_accuracy: 0.8034\n","102/102 [==============================] - 5s 26ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 28s 81ms/step - loss: 0.5141 - accuracy: 0.7510 - val_loss: 0.4019 - val_accuracy: 0.8279\n","Epoch 2/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.3618 - accuracy: 0.8576 - val_loss: 0.4341 - val_accuracy: 0.8262\n","Epoch 3/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.2482 - accuracy: 0.9129 - val_loss: 0.4643 - val_accuracy: 0.8288\n","Epoch 4/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1608 - accuracy: 0.9467 - val_loss: 0.5147 - val_accuracy: 0.8130\n","Epoch 5/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.1206 - accuracy: 0.9628 - val_loss: 0.5897 - val_accuracy: 0.8126\n","Epoch 6/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0898 - accuracy: 0.9673 - val_loss: 0.7424 - val_accuracy: 0.8244\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0803 - accuracy: 0.9719 - val_loss: 0.7806 - val_accuracy: 0.8017\n","Epoch 8/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0691 - accuracy: 0.9734 - val_loss: 0.8234 - val_accuracy: 0.8113\n","Epoch 9/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0482 - accuracy: 0.9818 - val_loss: 0.8339 - val_accuracy: 0.8113\n","Epoch 10/10\n","167/167 [==============================] - 11s 64ms/step - loss: 0.0519 - accuracy: 0.9801 - val_loss: 0.7409 - val_accuracy: 0.7859\n","102/102 [==============================] - 7s 26ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 28s 82ms/step - loss: 0.4969 - accuracy: 0.7735 - val_loss: 0.3807 - val_accuracy: 0.8393\n","Epoch 2/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.3665 - accuracy: 0.8591 - val_loss: 0.3907 - val_accuracy: 0.8301\n","Epoch 3/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.2712 - accuracy: 0.9024 - val_loss: 0.4030 - val_accuracy: 0.8314\n","Epoch 4/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.1905 - accuracy: 0.9375 - val_loss: 0.4588 - val_accuracy: 0.8266\n","Epoch 5/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.1422 - accuracy: 0.9508 - val_loss: 0.5672 - val_accuracy: 0.8275\n","Epoch 6/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0996 - accuracy: 0.9660 - val_loss: 0.7244 - val_accuracy: 0.8222\n","Epoch 7/10\n","167/167 [==============================] - 11s 65ms/step - loss: 0.0828 - accuracy: 0.9698 - val_loss: 0.7076 - val_accuracy: 0.8144\n","Epoch 8/10\n","167/167 [==============================] - 11s 67ms/step - loss: 0.0783 - accuracy: 0.9715 - val_loss: 0.8087 - val_accuracy: 0.8157\n","Epoch 9/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0623 - accuracy: 0.9769 - val_loss: 0.8697 - val_accuracy: 0.8126\n","Epoch 10/10\n","167/167 [==============================] - 11s 67ms/step - loss: 0.0422 - accuracy: 0.9809 - val_loss: 0.8945 - val_accuracy: 0.8104\n","102/102 [==============================] - 6s 28ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 29s 84ms/step - loss: 0.4979 - accuracy: 0.7692 - val_loss: 0.4243 - val_accuracy: 0.8266\n","Epoch 2/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.3627 - accuracy: 0.8604 - val_loss: 0.4063 - val_accuracy: 0.8266\n","Epoch 3/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.2614 - accuracy: 0.9099 - val_loss: 0.4756 - val_accuracy: 0.8266\n","Epoch 4/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.1787 - accuracy: 0.9384 - val_loss: 0.5276 - val_accuracy: 0.8039\n","Epoch 5/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.1413 - accuracy: 0.9506 - val_loss: 0.6186 - val_accuracy: 0.8227\n","Epoch 6/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.1042 - accuracy: 0.9621 - val_loss: 0.6515 - val_accuracy: 0.8262\n","Epoch 7/10\n","167/167 [==============================] - 11s 67ms/step - loss: 0.0810 - accuracy: 0.9715 - val_loss: 0.6905 - val_accuracy: 0.8227\n","Epoch 8/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0589 - accuracy: 0.9749 - val_loss: 0.7693 - val_accuracy: 0.8170\n","Epoch 9/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0524 - accuracy: 0.9801 - val_loss: 0.8134 - val_accuracy: 0.8179\n","Epoch 10/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0482 - accuracy: 0.9801 - val_loss: 0.8850 - val_accuracy: 0.8170\n","102/102 [==============================] - 5s 27ms/step\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","167/167 [==============================] - 30s 83ms/step - loss: 0.4853 - accuracy: 0.7857 - val_loss: 0.3749 - val_accuracy: 0.8481\n","Epoch 2/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.3571 - accuracy: 0.8598 - val_loss: 0.3945 - val_accuracy: 0.8463\n","Epoch 3/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.2695 - accuracy: 0.8994 - val_loss: 0.4920 - val_accuracy: 0.8231\n","Epoch 4/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.1802 - accuracy: 0.9383 - val_loss: 0.5082 - val_accuracy: 0.8109\n","Epoch 5/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.1290 - accuracy: 0.9576 - val_loss: 0.6233 - val_accuracy: 0.8174\n","Epoch 6/10\n","167/167 [==============================] - 11s 67ms/step - loss: 0.0868 - accuracy: 0.9683 - val_loss: 0.6077 - val_accuracy: 0.8004\n","Epoch 7/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0799 - accuracy: 0.9683 - val_loss: 0.7532 - val_accuracy: 0.8288\n","Epoch 8/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0683 - accuracy: 0.9709 - val_loss: 0.8809 - val_accuracy: 0.8034\n","Epoch 9/10\n","167/167 [==============================] - 11s 66ms/step - loss: 0.0564 - accuracy: 0.9769 - val_loss: 0.7559 - val_accuracy: 0.8319\n","Epoch 10/10\n","167/167 [==============================] - 11s 67ms/step - loss: 0.0559 - accuracy: 0.9782 - val_loss: 1.0040 - val_accuracy: 0.8183\n","102/102 [==============================] - 5s 28ms/step\n"]}]},{"cell_type":"code","source":["y_preds = np.array(y_preds)"],"metadata":{"id":"GEceTVsJuhcO","executionInfo":{"status":"ok","timestamp":1674930166739,"user_tz":300,"elapsed":6,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["results = (y_preds.mean(axis=0) > 0.5).astype('int')"],"metadata":{"id":"miRXa7M26Kcb","executionInfo":{"status":"ok","timestamp":1674930166739,"user_tz":300,"elapsed":5,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["output_df = pd.DataFrame({\n","    'id':test_df['id'],\n","    'target':results\n","})\n","\n","output_df.to_csv(path + \"submission3.csv\", index=False)"],"metadata":{"id":"mtMiDlOP6Ara","executionInfo":{"status":"ok","timestamp":1674930166740,"user_tz":300,"elapsed":6,"user":{"displayName":"Fuyuan Jing","userId":"01480936930571535587"}}},"execution_count":15,"outputs":[]}]}